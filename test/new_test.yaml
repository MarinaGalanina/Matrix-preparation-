apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:13:37Z"
    generateName: cilium-
    labels:
      controller-revision-hash: 66f9b7bf4f
      k8s-app: cilium
      pod-template-generation: "1"
    name: cilium-7f6mqF
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: 80b4a8a3-e331-40bf-a7d8-40d406ca20d2
    resourceVersion: "1276"
    uid: 703ec979-e1f4-4b51-bf99-a742f227d934
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-worker2
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - cilium
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: CILIUM_CNI_CHAINING_MODE
        valueFrom:
          configMapKeyRef:
            key: cni-chaining-mode
            name: cilium-config
            optional: true
      - name: CILIUM_CUSTOM_CNI_CONF
        valueFrom:
          configMapKeyRef:
            key: custom-cni-conf
            name: cilium-config
            optional: true
      image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - /cni-install.sh
            - --enable-debug=false
            - --cni-exclusive=true
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        privileged: true
      startupProbe:
        failureThreshold: 105
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: Bidirectional
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/lib/cilium/tls/hubble
        name: hubble-tls
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xjtl9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-mount /hostbin/cilium-mount;
        nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
        rm /hostbin/cilium-mount
      env:
      - name: CGROUP_ROOT
        value: /run/cilium/cgroupv2
      - name: BIN_PATH
        value: /opt/cni/bin
      image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      imagePullPolicy: IfNotPresent
      name: mount-cgroup
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xjtl9
        readOnly: true
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /run/cilium/cgroupv2
        mountPropagation: HostToContainer
        name: cilium-cgroup
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xjtl9
        readOnly: true
    nodeName: kind-worker2
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /proc
        type: Directory
      name: hostproc
    - hostPath:
        path: /run/cilium/cgroupv2
        type: DirectoryOrCreate
      name: cilium-cgroup
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      secret:
        defaultMode: 256
        optional: true
        secretName: cilium-clustermesh
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: hubble-tls
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: ca.crt
              path: client-ca.crt
            - key: tls.crt
              path: server.crt
            - key: tls.key
              path: server.key
            name: hubble-server-certs
            optional: true
    - name: kube-api-access-xjtl9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ec5515e9c21c7eee3a4e5f669f578763fc6a85b65cd10b807fcdeb6d68ad1f57
      image: sha256:1a7ecd2bef6dc2b8ecafd3e9afd47be2e19ed5f6430ccda59deb8ad8b992b28c
      imageID: quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      lastState: {}
      name: cilium-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:13:39Z"
    hostIP: 172.18.0.2
    initContainerStatuses:
    - containerID: containerd://2c4c6b42c9c0afbb4c533cbf9c646ad3c1e980a08c47d1bf47a88576fe3d05e6
      image: sha256:1a7ecd2bef6dc2b8ecafd3e9afd47be2e19ed5f6430ccda59deb8ad8b992b28c
      imageID: quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      lastState: {}
      name: mount-cgroup
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://2c4c6b42c9c0afbb4c533cbf9c646ad3c1e980a08c47d1bf47a88576fe3d05e6
          exitCode: 0
          finishedAt: "2022-07-14T20:13:37Z"
          reason: Completed
          startedAt: "2022-07-14T20:13:37Z"
    - containerID: containerd://7c399619827c5bf16cf465ed65ab09f49a25c1011d6a33ed4bf986587df58031
      image: sha256:1a7ecd2bef6dc2b8ecafd3e9afd47be2e19ed5f6430ccda59deb8ad8b992b28c
      imageID: quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      lastState: {}
      name: clean-cilium-state
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://7c399619827c5bf16cf465ed65ab09f49a25c1011d6a33ed4bf986587df58031
          exitCode: 0
          finishedAt: "2022-07-14T20:13:38Z"
          reason: Completed
          startedAt: "2022-07-14T20:13:38Z"
    phase: Running
    podIP: 172.18.0.2
    podIPs:
    - ip: 172.18.0.2
    qosClass: Burstable
    startTime: "2022-07-14T20:13:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:13:37Z"
    generateName: cilium-
    labels:
      controller-revision-hash: 66f9b7bf4f
      k8s-app: cilium
      pod-template-generation: "1"
    name: cilium-clq7h
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: 80b4a8a3-e331-40bf-a7d8-40d406ca20d2
    resourceVersion: "1277"
    uid: 5e25fe3f-243d-47d6-9d21-a9e8c1b6f41f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-control-plane
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - cilium
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: CILIUM_CNI_CHAINING_MODE
        valueFrom:
          configMapKeyRef:
            key: cni-chaining-mode
            name: cilium-config
            optional: true
      - name: CILIUM_CUSTOM_CNI_CONF
        valueFrom:
          configMapKeyRef:
            key: custom-cni-conf
            name: cilium-config
            optional: true
      image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - /cni-install.sh
            - --enable-debug=false
            - --cni-exclusive=true
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        privileged: true
      startupProbe:
        failureThreshold: 105
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: Bidirectional
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/lib/cilium/tls/hubble
        name: hubble-tls
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fr8wk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-mount /hostbin/cilium-mount;
        nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
        rm /hostbin/cilium-mount
      env:
      - name: CGROUP_ROOT
        value: /run/cilium/cgroupv2
      - name: BIN_PATH
        value: /opt/cni/bin
      image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      imagePullPolicy: IfNotPresent
      name: mount-cgroup
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fr8wk
        readOnly: true
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /run/cilium/cgroupv2
        mountPropagation: HostToContainer
        name: cilium-cgroup
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fr8wk
        readOnly: true
    nodeName: kind-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /proc
        type: Directory
      name: hostproc
    - hostPath:
        path: /run/cilium/cgroupv2
        type: DirectoryOrCreate
      name: cilium-cgroup
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      secret:
        defaultMode: 256
        optional: true
        secretName: cilium-clustermesh
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: hubble-tls
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: ca.crt
              path: client-ca.crt
            - key: tls.crt
              path: server.crt
            - key: tls.key
              path: server.key
            name: hubble-server-certs
            optional: true
    - name: kube-api-access-fr8wk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8aae37d60fdca2dd7cee3b2702b761c676e749dc199fc89c3f4350bfae930945
      image: sha256:1a7ecd2bef6dc2b8ecafd3e9afd47be2e19ed5f6430ccda59deb8ad8b992b28c
      imageID: quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      lastState: {}
      name: cilium-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:13:39Z"
    hostIP: 172.18.0.5
    initContainerStatuses:
    - containerID: containerd://96ffe570f03efeb45247dac00f77526bf2831d4c9b8bcf0b599cbf1c0fbb2469
      image: sha256:1a7ecd2bef6dc2b8ecafd3e9afd47be2e19ed5f6430ccda59deb8ad8b992b28c
      imageID: quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      lastState: {}
      name: mount-cgroup
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://96ffe570f03efeb45247dac00f77526bf2831d4c9b8bcf0b599cbf1c0fbb2469
          exitCode: 0
          finishedAt: "2022-07-14T20:13:37Z"
          reason: Completed
          startedAt: "2022-07-14T20:13:37Z"
    - containerID: containerd://b8e4745df779eadd8e2fad0e38aaaf32fb19c412972913b125724769f729f94d
      image: sha256:1a7ecd2bef6dc2b8ecafd3e9afd47be2e19ed5f6430ccda59deb8ad8b992b28c
      imageID: quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      lastState: {}
      name: clean-cilium-state
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://b8e4745df779eadd8e2fad0e38aaaf32fb19c412972913b125724769f729f94d
          exitCode: 0
          finishedAt: "2022-07-14T20:13:38Z"
          reason: Completed
          startedAt: "2022-07-14T20:13:38Z"
    phase: Running
    podIP: 172.18.0.5
    podIPs:
    - ip: 172.18.0.5
    qosClass: Burstable
    startTime: "2022-07-14T20:13:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:11:14Z"
    generateName: cilium-operator-5887f78bbb-
    labels:
      io.cilium/app: operator
      name: cilium-operator
      pod-template-hash: 5887f78bbb
    name: cilium-operator-5887f78bbb-2g6tc
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cilium-operator-5887f78bbb
      uid: c588419c-8024-44d5-a751-2228c4b6dd57
    resourceVersion: "987"
    uid: 375c93e3-6787-4b51-9ae0-a2e76f4b5bb7
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: io.cilium/app
              operator: In
              values:
              - operator
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      - --debug=$(CILIUM_DEBUG)
      command:
      - cilium-operator-generic
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_DEBUG
        valueFrom:
          configMapKeyRef:
            key: debug
            name: cilium-config
            optional: true
      image: quay.io/cilium/operator-generic:v1.11.6@sha256:9f6063c7bcaede801a39315ec7c166309f6a6783e98665f6693939cf1701bc17
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 9234
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: cilium-operator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-r482r
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-worker2
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium-operator
    serviceAccountName: cilium-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: kube-api-access-r482r
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:11:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:12:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:12:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:11:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://57af424eb284599a5e784eb2089b65f7386979c2ff1f6edfa680ff621924567c
      image: sha256:efddc1ae81f3b1e30e30f7b0ed223a1c4a810fe16c38f12831cb76e132c8aee1
      imageID: quay.io/cilium/operator-generic@sha256:9f6063c7bcaede801a39315ec7c166309f6a6783e98665f6693939cf1701bc17
      lastState: {}
      name: cilium-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:12:59Z"
    hostIP: 172.18.0.2
    phase: Running
    podIP: 172.18.0.2
    podIPs:
    - ip: 172.18.0.2
    qosClass: BestEffort
    startTime: "2022-07-14T20:11:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:13:38Z"
    generateName: cilium-
    labels:
      controller-revision-hash: 66f9b7bf4f
      k8s-app: cilium
      pod-template-generation: "1"
    name: cilium-r98jx
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: 80b4a8a3-e331-40bf-a7d8-40d406ca20d2
    resourceVersion: "1293"
    uid: f4013416-1437-48db-9540-4f3d255f6b6e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-worker
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - cilium
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: CILIUM_CNI_CHAINING_MODE
        valueFrom:
          configMapKeyRef:
            key: cni-chaining-mode
            name: cilium-config
            optional: true
      - name: CILIUM_CUSTOM_CNI_CONF
        valueFrom:
          configMapKeyRef:
            key: custom-cni-conf
            name: cilium-config
            optional: true
      image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - /cni-install.sh
            - --enable-debug=false
            - --cni-exclusive=true
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        privileged: true
      startupProbe:
        failureThreshold: 105
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: Bidirectional
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/lib/cilium/tls/hubble
        name: hubble-tls
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6q6s5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-mount /hostbin/cilium-mount;
        nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
        rm /hostbin/cilium-mount
      env:
      - name: CGROUP_ROOT
        value: /run/cilium/cgroupv2
      - name: BIN_PATH
        value: /opt/cni/bin
      image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      imagePullPolicy: IfNotPresent
      name: mount-cgroup
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6q6s5
        readOnly: true
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /run/cilium/cgroupv2
        mountPropagation: HostToContainer
        name: cilium-cgroup
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6q6s5
        readOnly: true
    nodeName: kind-worker
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /proc
        type: Directory
      name: hostproc
    - hostPath:
        path: /run/cilium/cgroupv2
        type: DirectoryOrCreate
      name: cilium-cgroup
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      secret:
        defaultMode: 256
        optional: true
        secretName: cilium-clustermesh
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: hubble-tls
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: ca.crt
              path: client-ca.crt
            - key: tls.crt
              path: server.crt
            - key: tls.key
              path: server.key
            name: hubble-server-certs
            optional: true
    - name: kube-api-access-6q6s5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:41Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:48Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:38Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8f9c78c56c2478fc30f638ab49928a6b86bee3d7366b87f7fa9833de721433fb
      image: sha256:1a7ecd2bef6dc2b8ecafd3e9afd47be2e19ed5f6430ccda59deb8ad8b992b28c
      imageID: quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      lastState: {}
      name: cilium-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:13:41Z"
    hostIP: 172.18.0.3
    initContainerStatuses:
    - containerID: containerd://fdf1b809cfec59613b5b8391aaddbe4359cba194d21a11c0c081618937ccb98b
      image: sha256:1a7ecd2bef6dc2b8ecafd3e9afd47be2e19ed5f6430ccda59deb8ad8b992b28c
      imageID: quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      lastState: {}
      name: mount-cgroup
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://fdf1b809cfec59613b5b8391aaddbe4359cba194d21a11c0c081618937ccb98b
          exitCode: 0
          finishedAt: "2022-07-14T20:13:39Z"
          reason: Completed
          startedAt: "2022-07-14T20:13:39Z"
    - containerID: containerd://ac21f70bce17e7d822deb8e16ad23b23d7b4952fd76f77e24e1d82871f50be80
      image: sha256:1a7ecd2bef6dc2b8ecafd3e9afd47be2e19ed5f6430ccda59deb8ad8b992b28c
      imageID: quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      lastState: {}
      name: clean-cilium-state
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://ac21f70bce17e7d822deb8e16ad23b23d7b4952fd76f77e24e1d82871f50be80
          exitCode: 0
          finishedAt: "2022-07-14T20:13:40Z"
          reason: Completed
          startedAt: "2022-07-14T20:13:40Z"
    phase: Running
    podIP: 172.18.0.3
    podIPs:
    - ip: 172.18.0.3
    qosClass: Burstable
    startTime: "2022-07-14T20:13:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:13:38Z"
    generateName: cilium-
    labels:
      controller-revision-hash: 66f9b7bf4f
      k8s-app: cilium
      pod-template-generation: "1"
    name: cilium-rg427
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: 80b4a8a3-e331-40bf-a7d8-40d406ca20d2
    resourceVersion: "1287"
    uid: 95309d79-91ad-4f9a-8e92-92fd78ca53ee
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-worker3
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - cilium
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: CILIUM_CNI_CHAINING_MODE
        valueFrom:
          configMapKeyRef:
            key: cni-chaining-mode
            name: cilium-config
            optional: true
      - name: CILIUM_CUSTOM_CNI_CONF
        valueFrom:
          configMapKeyRef:
            key: custom-cni-conf
            name: cilium-config
            optional: true
      image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - /cni-install.sh
            - --enable-debug=false
            - --cni-exclusive=true
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        privileged: true
      startupProbe:
        failureThreshold: 105
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: Bidirectional
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/lib/cilium/tls/hubble
        name: hubble-tls
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zghpt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-mount /hostbin/cilium-mount;
        nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
        rm /hostbin/cilium-mount
      env:
      - name: CGROUP_ROOT
        value: /run/cilium/cgroupv2
      - name: BIN_PATH
        value: /opt/cni/bin
      image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      imagePullPolicy: IfNotPresent
      name: mount-cgroup
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zghpt
        readOnly: true
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /run/cilium/cgroupv2
        mountPropagation: HostToContainer
        name: cilium-cgroup
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zghpt
        readOnly: true
    nodeName: kind-worker3
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /proc
        type: Directory
      name: hostproc
    - hostPath:
        path: /run/cilium/cgroupv2
        type: DirectoryOrCreate
      name: cilium-cgroup
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      secret:
        defaultMode: 256
        optional: true
        secretName: cilium-clustermesh
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: hubble-tls
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: ca.crt
              path: client-ca.crt
            - key: tls.crt
              path: server.crt
            - key: tls.key
              path: server.key
            name: hubble-server-certs
            optional: true
    - name: kube-api-access-zghpt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:46Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:46Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:38Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e735c0bbe96db3139ec922c0ad5b08e433411f992c516ebb7d48ff7a031e1e7f
      image: sha256:1a7ecd2bef6dc2b8ecafd3e9afd47be2e19ed5f6430ccda59deb8ad8b992b28c
      imageID: quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      lastState: {}
      name: cilium-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:13:40Z"
    hostIP: 172.18.0.4
    initContainerStatuses:
    - containerID: containerd://7391b66c805517409c4cc5c860a63570c6d7adc13a52e6a820245fcf0a0d12ec
      image: sha256:1a7ecd2bef6dc2b8ecafd3e9afd47be2e19ed5f6430ccda59deb8ad8b992b28c
      imageID: quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      lastState: {}
      name: mount-cgroup
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://7391b66c805517409c4cc5c860a63570c6d7adc13a52e6a820245fcf0a0d12ec
          exitCode: 0
          finishedAt: "2022-07-14T20:13:39Z"
          reason: Completed
          startedAt: "2022-07-14T20:13:39Z"
    - containerID: containerd://81670d578329be9090d1c26305fd25155c68df5d1041480146ca8075ddd56630
      image: sha256:1a7ecd2bef6dc2b8ecafd3e9afd47be2e19ed5f6430ccda59deb8ad8b992b28c
      imageID: quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      lastState: {}
      name: clean-cilium-state
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://81670d578329be9090d1c26305fd25155c68df5d1041480146ca8075ddd56630
          exitCode: 0
          finishedAt: "2022-07-14T20:13:39Z"
          reason: Completed
          startedAt: "2022-07-14T20:13:39Z"
    phase: Running
    podIP: 172.18.0.4
    podIPs:
    - ip: 172.18.0.4
    qosClass: Burstable
    startTime: "2022-07-14T20:13:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:11:15Z"
    generateName: coredns-6d4b75cb6d-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 6d4b75cb6d
    name: coredns-6d4b75cb6d-f2ctq
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-6d4b75cb6d
      uid: 779cf116-ce79-4ae3-9c2f-f207591351fa
    resourceVersion: "1100"
    uid: 6c5eb1c7-a00d-473b-90b4-e327d2f13751
  spec:
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: k8s.gcr.io/coredns/coredns:v1.8.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6xrr
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: kind-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-t6xrr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:12:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:12:53Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://caf2a16d542baab4141fa35bcc7a98c663c331ccd242a58e57190a7202d41753
      image: k8s.gcr.io/coredns/coredns:v1.8.6
      imageID: sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:13:18Z"
    hostIP: 172.18.0.5
    phase: Running
    podIP: 10.244.0.51
    podIPs:
    - ip: 10.244.0.51
    qosClass: Burstable
    startTime: "2022-07-14T20:12:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:11:14Z"
    generateName: coredns-6d4b75cb6d-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 6d4b75cb6d
    name: coredns-6d4b75cb6d-gt47v
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-6d4b75cb6d
      uid: 779cf116-ce79-4ae3-9c2f-f207591351fa
    resourceVersion: "1091"
    uid: 4b0e6dca-3b3c-4db6-9000-d44cc7cf672e
  spec:
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: k8s.gcr.io/coredns/coredns:v1.8.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5jgkk
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: kind-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-5jgkk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:12:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:12:53Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0357ee30a56b1b9b9f4d3b949d44041f7ae84a57a3de8f5b7f5089375396f498
      image: k8s.gcr.io/coredns/coredns:v1.8.6
      imageID: sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:13:18Z"
    hostIP: 172.18.0.5
    phase: Running
    podIP: 10.244.0.3
    podIPs:
    - ip: 10.244.0.3
    qosClass: Burstable
    startTime: "2022-07-14T20:12:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.5:2379
      kubernetes.io/config.hash: 5d8cf2c6783115bd6149d11d97830083
      kubernetes.io/config.mirror: 5d8cf2c6783115bd6149d11d97830083
      kubernetes.io/config.seen: "2022-07-14T20:09:08.366354630Z"
      kubernetes.io/config.source: file
      seccomp.security.alpha.kubernetes.io/pod: runtime/default
    creationTimestamp: "2022-07-14T20:09:09Z"
    labels:
      component: etcd
      tier: control-plane
    name: etcd-kind-control-plane
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: kind-control-plane
      uid: c2d1e453-3eb5-45e3-955a-8d9efe36246b
    resourceVersion: "285"
    uid: b88940ac-1029-4df3-a1a6-9afc16acb257
  spec:
    containers:
    - command:
      - etcd
      - --advertise-client-urls=https://172.18.0.5:2379
      - --cert-file=/etc/kubernetes/pki/etcd/server.crt
      - --client-cert-auth=true
      - --data-dir=/var/lib/etcd
      - --experimental-initial-corrupt-check=true
      - --initial-advertise-peer-urls=https://172.18.0.5:2380
      - --initial-cluster=kind-control-plane=https://172.18.0.5:2380
      - --key-file=/etc/kubernetes/pki/etcd/server.key
      - --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.5:2379
      - --listen-metrics-urls=http://127.0.0.1:2381
      - --listen-peer-urls=https://172.18.0.5:2380
      - --name=kind-control-plane
      - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      - --peer-client-cert-auth=true
      - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      - --snapshot-count=10000
      - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      image: k8s.gcr.io/etcd:3.5.3-0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /health
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: etcd
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /health
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/etcd
        name: etcd-data
      - mountPath: /etc/kubernetes/pki/etcd
        name: etcd-certs
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/pki/etcd
        type: DirectoryOrCreate
      name: etcd-certs
    - hostPath:
        path: /var/lib/etcd
        type: DirectoryOrCreate
      name: etcd-data
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://39469eed98b4da51b51b0308974bbb326cab9c877a7c5cbadcc6fb685669cc80
      image: k8s.gcr.io/etcd:3.5.3-0
      imageID: sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
      lastState: {}
      name: etcd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:08:57Z"
    hostIP: 172.18.0.5
    phase: Running
    podIP: 172.18.0.5
    podIPs:
    - ip: 172.18.0.5
    qosClass: Burstable
    startTime: "2022-07-14T20:09:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:13:54Z"
    generateName: hubble-relay-959988db5-
    labels:
      k8s-app: hubble-relay
      pod-template-hash: 959988db5
    name: hubble-relay-959988db5-f46rg
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hubble-relay-959988db5
      uid: 2f541e7f-67dc-4ce3-9383-55cb0f721e47
    resourceVersion: "1369"
    uid: 3750754d-6d8e-4e21-a6d4-2a523495e920
  spec:
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - cilium
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: false
    containers:
    - args:
      - serve
      command:
      - hubble-relay
      image: quay.io/cilium/hubble-relay:v1.11.6@sha256:fd9034a2d04d5b973f1e8ed44f230ea195b89c37955ff32e34e5aa68f3ed675a
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: grpc
        timeoutSeconds: 1
      name: hubble-relay
      ports:
      - containerPort: 4245
        name: grpc
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: grpc
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/hubble-relay
        name: config
        readOnly: true
      - mountPath: /var/lib/hubble-relay/tls
        name: tls
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kind-worker3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: hubble-relay
    serviceAccountName: hubble-relay
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: hubble-relay-config
      name: config
    - name: tls
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: ca.crt
              path: hubble-server-ca.crt
            - key: tls.crt
              path: client.crt
            - key: tls.key
              path: client.key
            name: hubble-relay-client-certs
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:14:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:14:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c54838136973cb637f73b09c46f847c562bc494a598dd084b083f224e8abe6ce
      image: sha256:f629a56d0afc21f1ae25fa3713274626e9784cdac89be9165dcf553174d078ec
      imageID: quay.io/cilium/hubble-relay@sha256:fd9034a2d04d5b973f1e8ed44f230ea195b89c37955ff32e34e5aa68f3ed675a
      lastState: {}
      name: hubble-relay
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:14:08Z"
    hostIP: 172.18.0.4
    phase: Running
    podIP: 10.244.3.215
    podIPs:
    - ip: 10.244.3.215
    qosClass: BestEffort
    startTime: "2022-07-14T20:13:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.5:6443
      kubernetes.io/config.hash: 282f94df266ad93b73f94e4a715c85ef
      kubernetes.io/config.mirror: 282f94df266ad93b73f94e4a715c85ef
      kubernetes.io/config.seen: "2022-07-14T20:09:08.366360155Z"
      kubernetes.io/config.source: file
      seccomp.security.alpha.kubernetes.io/pod: runtime/default
    creationTimestamp: "2022-07-14T20:09:08Z"
    labels:
      component: kube-apiserver
      tier: control-plane
    name: kube-apiserver-kind-control-plane
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: kind-control-plane
      uid: c2d1e453-3eb5-45e3-955a-8d9efe36246b
    resourceVersion: "287"
    uid: 342b832a-c7b3-4a98-a4f0-2f6b053160e0
  spec:
    containers:
    - command:
      - kube-apiserver
      - --advertise-address=172.18.0.5
      - --allow-privileged=true
      - --authorization-mode=Node,RBAC
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --enable-admission-plugins=NodeRestriction
      - --enable-bootstrap-token-auth=true
      - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      - --etcd-servers=https://127.0.0.1:2379
      - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      - --requestheader-allowed-names=front-proxy-client
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --requestheader-extra-headers-prefix=X-Remote-Extra-
      - --requestheader-group-headers=X-Remote-Group
      - --requestheader-username-headers=X-Remote-User
      - --runtime-config=
      - --secure-port=6443
      - --service-account-issuer=https://kubernetes.default.svc.cluster.local
      - --service-account-key-file=/etc/kubernetes/pki/sa.pub
      - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=10.96.0.0/16
      - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
      env:
      - name: HTTPS_PROXY
        value: http://135.245.192.7:8000
      - name: https_proxy
        value: http://135.245.192.7:8000
      - name: NO_PROXY
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: no_proxy
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: HTTP_PROXY
        value: http://135.245.192.7:8000
      - name: http_proxy
        value: http://135.245.192.7:8000
      image: k8s.gcr.io/kube-apiserver:v1.24.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 172.18.0.5
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-apiserver
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 172.18.0.5
          path: /readyz
          port: 6443
          scheme: HTTPS
        periodSeconds: 1
        successThreshold: 1
        timeoutSeconds: 15
      resources:
        requests:
          cpu: 250m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 172.18.0.5
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7ce1ef7f7804540cbd62cb5ef7f445dcdbdf95740af63ccd70ee1cd68301b89c
      image: k8s.gcr.io/kube-apiserver:v1.24.0
      imageID: docker.io/library/import-2022-05-19@sha256:5611470386fb81d6d169bb37667ae1f9fa884631b265b54d8d9601d9338aad18
      lastState: {}
      name: kube-apiserver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:08:52Z"
    hostIP: 172.18.0.5
    phase: Running
    podIP: 172.18.0.5
    podIPs:
    - ip: 172.18.0.5
    qosClass: Burstable
    startTime: "2022-07-14T20:09:09Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 7a26ad1d2ee33d26b3e92c21eb3b261b
      kubernetes.io/config.mirror: 7a26ad1d2ee33d26b3e92c21eb3b261b
      kubernetes.io/config.seen: "2022-07-14T20:09:08.366361922Z"
      kubernetes.io/config.source: file
      seccomp.security.alpha.kubernetes.io/pod: runtime/default
    creationTimestamp: "2022-07-14T20:09:08Z"
    labels:
      component: kube-controller-manager
      tier: control-plane
    name: kube-controller-manager-kind-control-plane
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: kind-control-plane
      uid: c2d1e453-3eb5-45e3-955a-8d9efe36246b
    resourceVersion: "296"
    uid: 9bd7b108-3746-4598-9d66-ed359e580b79
  spec:
    containers:
    - command:
      - kube-controller-manager
      - --allocate-node-cidrs=true
      - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --bind-address=127.0.0.1
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --cluster-cidr=10.244.0.0/16
      - --cluster-name=kind
      - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      - --controllers=*,bootstrapsigner,tokencleaner
      - --enable-hostpath-provisioner=true
      - --kubeconfig=/etc/kubernetes/controller-manager.conf
      - --leader-elect=true
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --root-ca-file=/etc/kubernetes/pki/ca.crt
      - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=10.96.0.0/16
      - --use-service-account-credentials=true
      env:
      - name: HTTPS_PROXY
        value: http://135.245.192.7:8000
      - name: https_proxy
        value: http://135.245.192.7:8000
      - name: NO_PROXY
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: no_proxy
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: HTTP_PROXY
        value: http://135.245.192.7:8000
      - name: http_proxy
        value: http://135.245.192.7:8000
      image: k8s.gcr.io/kube-controller-manager:v1.24.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-controller-manager
      resources:
        requests:
          cpu: 200m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        name: flexvolume-dir
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /etc/kubernetes/controller-manager.conf
        name: kubeconfig
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        type: DirectoryOrCreate
      name: flexvolume-dir
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /etc/kubernetes/controller-manager.conf
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://225a7eeabadff45a65c69278783d4764c02706790e3770f94babd93d5095ceb6
      image: k8s.gcr.io/kube-controller-manager:v1.24.0
      imageID: docker.io/library/import-2022-05-19@sha256:948ca7c1da4b6108193f4dae34f2e94cfdaa901ab8876ba4009cc024673d2af4
      lastState: {}
      name: kube-controller-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:08:52Z"
    hostIP: 172.18.0.5
    phase: Running
    podIP: 172.18.0.5
    podIPs:
    - ip: 172.18.0.5
    qosClass: Burstable
    startTime: "2022-07-14T20:09:09Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:09:26Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 59c4c7b696
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-8svhz
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: 61c216a8-b0ee-4e1d-853b-fae68d2b0626
    resourceVersion: "488"
    uid: aaf612e6-df13-4bc4-b220-77fc5615abb8
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-worker2
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: HTTPS_PROXY
        value: http://135.245.192.7:8000
      - name: https_proxy
        value: http://135.245.192.7:8000
      - name: NO_PROXY
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: no_proxy
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: HTTP_PROXY
        value: http://135.245.192.7:8000
      - name: http_proxy
        value: http://135.245.192.7:8000
      image: k8s.gcr.io/kube-proxy:v1.24.0
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k7bqn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-worker2
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-k7bqn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1813ef196e22583fca0f94dcabcb5adea27e8be57aa1844670889c210fb721c7
      image: k8s.gcr.io/kube-proxy:v1.24.0
      imageID: docker.io/library/import-2022-05-19@sha256:654122cbd980cb4a8417347b8c84ecdbc81a735d3d01409bec6f37ef293511ca
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:09:32Z"
    hostIP: 172.18.0.2
    phase: Running
    podIP: 172.18.0.2
    podIPs:
    - ip: 172.18.0.2
    qosClass: BestEffort
    startTime: "2022-07-14T20:09:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:09:21Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 59c4c7b696
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-hmb8j
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: 61c216a8-b0ee-4e1d-853b-fae68d2b0626
    resourceVersion: "386"
    uid: 24b0dc94-d71a-4110-8090-9d4a0ad15d8f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-control-plane
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: HTTPS_PROXY
        value: http://135.245.192.7:8000
      - name: https_proxy
        value: http://135.245.192.7:8000
      - name: NO_PROXY
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: no_proxy
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: HTTP_PROXY
        value: http://135.245.192.7:8000
      - name: http_proxy
        value: http://135.245.192.7:8000
      image: k8s.gcr.io/kube-proxy:v1.24.0
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xgg8n
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-xgg8n
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a59195fb25f66423ff6a1dd31afb14a99c0aaf1eb3acff60cb76e5b073a751fb
      image: k8s.gcr.io/kube-proxy:v1.24.0
      imageID: docker.io/library/import-2022-05-19@sha256:654122cbd980cb4a8417347b8c84ecdbc81a735d3d01409bec6f37ef293511ca
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:09:23Z"
    hostIP: 172.18.0.5
    phase: Running
    podIP: 172.18.0.5
    podIPs:
    - ip: 172.18.0.5
    qosClass: BestEffort
    startTime: "2022-07-14T20:09:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:09:28Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 59c4c7b696
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-n6q77
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: 61c216a8-b0ee-4e1d-853b-fae68d2b0626
    resourceVersion: "493"
    uid: 8160be04-c594-4f55-88f0-85466bf30215
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-worker3
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: HTTPS_PROXY
        value: http://135.245.192.7:8000
      - name: https_proxy
        value: http://135.245.192.7:8000
      - name: NO_PROXY
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: no_proxy
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: HTTP_PROXY
        value: http://135.245.192.7:8000
      - name: http_proxy
        value: http://135.245.192.7:8000
      image: k8s.gcr.io/kube-proxy:v1.24.0
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-25ksq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-worker3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-25ksq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a6746f83ac59eeeb0077a9e01afcec80e9f4ba28d41f648d1c762a24d5fe65d4
      image: k8s.gcr.io/kube-proxy:v1.24.0
      imageID: docker.io/library/import-2022-05-19@sha256:654122cbd980cb4a8417347b8c84ecdbc81a735d3d01409bec6f37ef293511ca
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:09:33Z"
    hostIP: 172.18.0.4
    phase: Running
    podIP: 172.18.0.4
    podIPs:
    - ip: 172.18.0.4
    qosClass: BestEffort
    startTime: "2022-07-14T20:09:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:09:25Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 59c4c7b696
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-qpnwh
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: 61c216a8-b0ee-4e1d-853b-fae68d2b0626
    resourceVersion: "485"
    uid: 4f507e3c-0a5c-45c5-80c0-ca0268b6ad92
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-worker
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: HTTPS_PROXY
        value: http://135.245.192.7:8000
      - name: https_proxy
        value: http://135.245.192.7:8000
      - name: NO_PROXY
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: no_proxy
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: HTTP_PROXY
        value: http://135.245.192.7:8000
      - name: http_proxy
        value: http://135.245.192.7:8000
      image: k8s.gcr.io/kube-proxy:v1.24.0
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-drkhd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-worker
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-drkhd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://144defb7a4d4f1d6b9faa6a656ba2d9ca55a9715ac878bdb32352893be212eab
      image: k8s.gcr.io/kube-proxy:v1.24.0
      imageID: docker.io/library/import-2022-05-19@sha256:654122cbd980cb4a8417347b8c84ecdbc81a735d3d01409bec6f37ef293511ca
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:09:31Z"
    hostIP: 172.18.0.3
    phase: Running
    podIP: 172.18.0.3
    podIPs:
    - ip: 172.18.0.3
    qosClass: BestEffort
    startTime: "2022-07-14T20:09:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: a348a8a7007bb2671ef14b9ccb418c8b
      kubernetes.io/config.mirror: a348a8a7007bb2671ef14b9ccb418c8b
      kubernetes.io/config.seen: "2022-07-14T20:09:08.366363314Z"
      kubernetes.io/config.source: file
      seccomp.security.alpha.kubernetes.io/pod: runtime/default
    creationTimestamp: "2022-07-14T20:09:08Z"
    labels:
      component: kube-scheduler
      tier: control-plane
    name: kube-scheduler-kind-control-plane
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: kind-control-plane
      uid: c2d1e453-3eb5-45e3-955a-8d9efe36246b
    resourceVersion: "289"
    uid: 265ad0c9-92b1-412e-b882-2069511daea1
  spec:
    containers:
    - command:
      - kube-scheduler
      - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      - --bind-address=127.0.0.1
      - --kubeconfig=/etc/kubernetes/scheduler.conf
      - --leader-elect=true
      env:
      - name: HTTPS_PROXY
        value: http://135.245.192.7:8000
      - name: https_proxy
        value: http://135.245.192.7:8000
      - name: NO_PROXY
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: no_proxy
        value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
      - name: HTTP_PROXY
        value: http://135.245.192.7:8000
      - name: http_proxy
        value: http://135.245.192.7:8000
      image: k8s.gcr.io/kube-scheduler:v1.24.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-scheduler
      resources:
        requests:
          cpu: 100m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/scheduler.conf
        name: kubeconfig
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/scheduler.conf
        type: FileOrCreate
      name: kubeconfig
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:09:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://24a4a2ff5a44d5260e17e6139b26e90a89abfdc0619ae7ff87e753e81ac7086c
      image: k8s.gcr.io/kube-scheduler:v1.24.0
      imageID: docker.io/library/import-2022-05-19@sha256:289df0671b753c90e6b717b92ada9af9bcb48678d59affc8cc27eef4f01e9251
      lastState: {}
      name: kube-scheduler
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:08:52Z"
    hostIP: 172.18.0.5
    phase: Running
    podIP: 172.18.0.5
    podIPs:
    - ip: 172.18.0.5
    qosClass: Burstable
    startTime: "2022-07-14T20:09:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:11:15Z"
    generateName: local-path-provisioner-9cd9bd544-
    labels:
      app: local-path-provisioner
      pod-template-hash: 9cd9bd544
    name: local-path-provisioner-9cd9bd544-sflb7
    namespace: local-path-storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-provisioner-9cd9bd544
      uid: e5ebfec0-e8a6-4fff-b75c-057862dce649
    resourceVersion: "1089"
    uid: 8a5a3b6e-9e3e-449c-bb86-ca5cb6c56242
  spec:
    containers:
    - command:
      - local-path-provisioner
      - --debug
      - start
      - --helper-image
      - docker.io/kindest/local-path-helper:v20220512-507ff70b
      - --config
      - /etc/config/config.json
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kgxdz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kind-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-provisioner-service-account
    serviceAccountName: local-path-provisioner-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Equal
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Equal
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-kgxdz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:12:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:13:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:12:53Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://457768a0c48f1349763deb8d04523560e3d82cebe0a362bfd26b8fd456c7bb9a
      image: docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
      imageID: sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
      lastState: {}
      name: local-path-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:13:18Z"
    hostIP: 172.18.0.5
    phase: Running
    podIP: 10.244.0.210
    podIPs:
    - ip: 10.244.0.210
    qosClass: BestEffort
    startTime: "2022-07-14T20:12:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:15:33Z"
    generateName: echo-compaas-echoserver-test-f89ffd6c9-
    labels:
      app: compaas-echoserver-test
      pod-template-hash: f89ffd6c9
      release: echo
    name: echo-compaas-echoserver-test-f89ffd6c9-kntnt
    namespace: namespace-a
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: echo-compaas-echoserver-test-f89ffd6c9
      uid: 24ca6a06-8571-444a-b3d8-5cb538595cf2
    resourceVersion: "1665"
    uid: 3cfa7610-3fdf-4057-abcb-c50b344f76b2
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - echo-compaas-echoserver-test
            topologyKey: kubernetes.io/hostname
          weight: 100
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - echo-compaas-echoserver-test
            topologyKey: failure-domain.beta.kubernetes.io/zone
          weight: 50
    containers:
    - image: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
      imagePullPolicy: IfNotPresent
      name: echoserver
      resources:
        limits:
          cpu: 15m
          memory: 12Mi
        requests:
          cpu: 2m
          memory: 6Mi
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /certs
        name: certs
      - mountPath: /var/tmp/nginx
        name: nginx-tmp
      - mountPath: /run/nginx
        name: nginx-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rckh4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kind-worker
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: certs
    - emptyDir: {}
      name: nginx-tmp
    - emptyDir: {}
      name: nginx-run
    - name: kube-api-access-rckh4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:15:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:15:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:15:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:15:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b980a9e40d6e266ac5f65fe811dce948bcb027d8cd318046e1f63a9150be6a61
      image: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
      imageID: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test@sha256:1361ea03466a6ea07a1607d73cae74cb3214961411e36867a5ffb113b7e15cd0
      lastState: {}
      name: echoserver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:15:51Z"
    hostIP: 172.18.0.3
    phase: Running
    podIP: 10.244.1.95
    podIPs:
    - ip: 10.244.1.95
    qosClass: Burstable
    startTime: "2022-07-14T20:15:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:15:42Z"
    generateName: echo-compaas-echoserver-test-f89ffd6c9-
    labels:
      app: compaas-echoserver-test
      pod-template-hash: f89ffd6c9
      release: echo
    name: echo-compaas-echoserver-test-f89ffd6c9-xwjtk
    namespace: namespace-b
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: echo-compaas-echoserver-test-f89ffd6c9
      uid: d669fc35-5989-419c-ac0b-935a01e25389
    resourceVersion: "1669"
    uid: b7bd97d0-a25d-48f4-be3c-b4b5958d409c
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - echo-compaas-echoserver-test
            topologyKey: kubernetes.io/hostname
          weight: 100
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - echo-compaas-echoserver-test
            topologyKey: failure-domain.beta.kubernetes.io/zone
          weight: 50
    containers:
    - image: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
      imagePullPolicy: IfNotPresent
      name: echoserver
      resources:
        limits:
          cpu: 15m
          memory: 12Mi
        requests:
          cpu: 2m
          memory: 6Mi
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /certs
        name: certs
      - mountPath: /var/tmp/nginx
        name: nginx-tmp
      - mountPath: /run/nginx
        name: nginx-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7rdvj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kind-worker
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: certs
    - emptyDir: {}
      name: nginx-tmp
    - emptyDir: {}
      name: nginx-run
    - name: kube-api-access-7rdvj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:15:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:15:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:15:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:15:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6caf03247d23ec7374213182b2760841139fc56c9229f2cd9bd4fcbcb93ebf3e
      image: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
      imageID: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test@sha256:1361ea03466a6ea07a1607d73cae74cb3214961411e36867a5ffb113b7e15cd0
      lastState: {}
      name: echoserver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:15:51Z"
    hostIP: 172.18.0.3
    phase: Running
    podIP: 10.244.1.1
    podIPs:
    - ip: 10.244.1.1
    qosClass: Burstable
    startTime: "2022-07-14T20:15:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-07-14T20:18:24Z"
    generateName: echo-compaas-echoserver-test-f89ffd6c9-
    labels:
      app: compaas-echoserver-test
      pod-template-hash: f89ffd6c9
      release: echo
    name: echo-compaas-echoserver-test-f89ffd6c9-mzk5b
    namespace: namespace-c
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: echo-compaas-echoserver-test-f89ffd6c9
      uid: 0d45baf2-1845-4b5b-a207-068e996265e6
    resourceVersion: "2059"
    uid: 561b31e7-0503-40a1-ab02-995667943ba7
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - echo-compaas-echoserver-test
            topologyKey: kubernetes.io/hostname
          weight: 100
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - echo-compaas-echoserver-test
            topologyKey: failure-domain.beta.kubernetes.io/zone
          weight: 50
    containers:
    - image: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
      imagePullPolicy: IfNotPresent
      name: echoserver
      resources:
        limits:
          cpu: 15m
          memory: 12Mi
        requests:
          cpu: 2m
          memory: 6Mi
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /certs
        name: certs
      - mountPath: /var/tmp/nginx
        name: nginx-tmp
      - mountPath: /run/nginx
        name: nginx-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zdqsx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kind-worker
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: certs
    - emptyDir: {}
      name: nginx-tmp
    - emptyDir: {}
      name: nginx-run
    - name: kube-api-access-zdqsx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:18:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:18:41Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:18:41Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-07-14T20:18:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1a0c4239c654fcea409e540567fc57079734b4360140cd5d492a9cacb8a9624f
      image: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
      imageID: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test@sha256:1361ea03466a6ea07a1607d73cae74cb3214961411e36867a5ffb113b7e15cd0
      lastState: {}
      name: echoserver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-07-14T20:18:41Z"
    hostIP: 172.18.0.3
    phase: Running
    podIP: 10.244.1.39
    podIPs:
    - ip: 10.244.1.39
    qosClass: Burstable
    startTime: "2022-07-14T20:18:24Z"
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-07-14T20:11:14Z"
    generation: 1
    labels:
      io.cilium/app: operator
      name: cilium-operator
      pod-template-hash: 5887f78bbb
    name: cilium-operator-5887f78bbb
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cilium-operator
      uid: 7880b5f2-e588-4ff4-95ce-65d34fb7afb0
    resourceVersion: "988"
    uid: c588419c-8024-44d5-a751-2228c4b6dd57
  spec:
    replicas: 1
    selector:
      matchLabels:
        io.cilium/app: operator
        name: cilium-operator
        pod-template-hash: 5887f78bbb
    template:
      metadata:
        creationTimestamp: null
        labels:
          io.cilium/app: operator
          name: cilium-operator
          pod-template-hash: 5887f78bbb
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: io.cilium/app
                  operator: In
                  values:
                  - operator
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --config-dir=/tmp/cilium/config-map
          - --debug=$(CILIUM_DEBUG)
          command:
          - cilium-operator-generic
          env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CILIUM_K8S_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CILIUM_DEBUG
            valueFrom:
              configMapKeyRef:
                key: debug
                name: cilium-config
                optional: true
          image: quay.io/cilium/operator-generic:v1.11.6@sha256:9f6063c7bcaede801a39315ec7c166309f6a6783e98665f6693939cf1701bc17
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9234
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: cilium-operator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/cilium/config-map
            name: cilium-config-path
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cilium-operator
        serviceAccountName: cilium-operator
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: cilium-config
          name: cilium-config-path
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-07-14T20:09:21Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 6d4b75cb6d
    name: coredns-6d4b75cb6d
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: b6f1a4d0-56a9-4150-8e88-656221bc88b5
    resourceVersion: "1103"
    uid: 779cf116-ce79-4ae3-9c2f-f207591351fa
  spec:
    replicas: 2
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 6d4b75cb6d
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 6d4b75cb6d
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: k8s.gcr.io/coredns/coredns:v1.8.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-07-14T20:13:54Z"
    generation: 1
    labels:
      k8s-app: hubble-relay
      pod-template-hash: 959988db5
    name: hubble-relay-959988db5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hubble-relay
      uid: 4da5a46e-2552-40a1-bfa9-102e3ede194c
    resourceVersion: "1372"
    uid: 2f541e7f-67dc-4ce3-9383-55cb0f721e47
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: hubble-relay
        pod-template-hash: 959988db5
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: hubble-relay
          pod-template-hash: 959988db5
      spec:
        affinity:
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: k8s-app
                  operator: In
                  values:
                  - cilium
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: false
        containers:
        - args:
          - serve
          command:
          - hubble-relay
          image: quay.io/cilium/hubble-relay:v1.11.6@sha256:fd9034a2d04d5b973f1e8ed44f230ea195b89c37955ff32e34e5aa68f3ed675a
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: grpc
            timeoutSeconds: 1
          name: hubble-relay
          ports:
          - containerPort: 4245
            name: grpc
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: grpc
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/hubble-relay
            name: config
            readOnly: true
          - mountPath: /var/lib/hubble-relay/tls
            name: tls
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hubble-relay
        serviceAccountName: hubble-relay
        terminationGracePeriodSeconds: 0
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: config.yaml
              path: config.yaml
            name: hubble-relay-config
          name: config
        - name: tls
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: ca.crt
                  path: hubble-server-ca.crt
                - key: tls.crt
                  path: client.crt
                - key: tls.key
                  path: client.key
                name: hubble-relay-client-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-07-14T20:09:21Z"
    generation: 1
    labels:
      app: local-path-provisioner
      pod-template-hash: 9cd9bd544
    name: local-path-provisioner-9cd9bd544
    namespace: local-path-storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: local-path-provisioner
      uid: d489f0c5-bcb8-4a27-8726-11ca3bb74617
    resourceVersion: "1090"
    uid: e5ebfec0-e8a6-4fff-b75c-057862dce649
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: local-path-provisioner
        pod-template-hash: 9cd9bd544
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
          pod-template-hash: 9cd9bd544
      spec:
        containers:
        - command:
          - local-path-provisioner
          - --debug
          - start
          - --helper-image
          - docker.io/kindest/local-path-helper:v20220512-507ff70b
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Equal
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Equal
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: echo
      meta.helm.sh/release-namespace: namespace-a
    creationTimestamp: "2022-07-14T20:15:33Z"
    generation: 1
    labels:
      app: compaas-echoserver-test
      pod-template-hash: f89ffd6c9
      release: echo
    name: echo-compaas-echoserver-test-f89ffd6c9
    namespace: namespace-a
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: echo-compaas-echoserver-test
      uid: 5521da30-c2fa-453c-9e79-74e4928aeed4
    resourceVersion: "1668"
    uid: 24ca6a06-8571-444a-b3d8-5cb538595cf2
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: compaas-echoserver-test
        pod-template-hash: f89ffd6c9
        release: echo
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: compaas-echoserver-test
          pod-template-hash: f89ffd6c9
          release: echo
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - echo-compaas-echoserver-test
                topologyKey: kubernetes.io/hostname
              weight: 100
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - echo-compaas-echoserver-test
                topologyKey: failure-domain.beta.kubernetes.io/zone
              weight: 50
        containers:
        - image: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
          imagePullPolicy: IfNotPresent
          name: echoserver
          resources:
            limits:
              cpu: 15m
              memory: 12Mi
            requests:
              cpu: 2m
              memory: 6Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certs
          - mountPath: /var/tmp/nginx
            name: nginx-tmp
          - mountPath: /run/nginx
            name: nginx-run
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: certs
        - emptyDir: {}
          name: nginx-tmp
        - emptyDir: {}
          name: nginx-run
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: echo
      meta.helm.sh/release-namespace: namespace-b
    creationTimestamp: "2022-07-14T20:15:42Z"
    generation: 1
    labels:
      app: compaas-echoserver-test
      pod-template-hash: f89ffd6c9
      release: echo
    name: echo-compaas-echoserver-test-f89ffd6c9
    namespace: namespace-b
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: echo-compaas-echoserver-test
      uid: f91d84c4-d398-43b6-9468-2859287d1df3
    resourceVersion: "1673"
    uid: d669fc35-5989-419c-ac0b-935a01e25389
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: compaas-echoserver-test
        pod-template-hash: f89ffd6c9
        release: echo
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: compaas-echoserver-test
          pod-template-hash: f89ffd6c9
          release: echo
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - echo-compaas-echoserver-test
                topologyKey: kubernetes.io/hostname
              weight: 100
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - echo-compaas-echoserver-test
                topologyKey: failure-domain.beta.kubernetes.io/zone
              weight: 50
        containers:
        - image: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
          imagePullPolicy: IfNotPresent
          name: echoserver
          resources:
            limits:
              cpu: 15m
              memory: 12Mi
            requests:
              cpu: 2m
              memory: 6Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certs
          - mountPath: /var/tmp/nginx
            name: nginx-tmp
          - mountPath: /run/nginx
            name: nginx-run
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: certs
        - emptyDir: {}
          name: nginx-tmp
        - emptyDir: {}
          name: nginx-run
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: echo
      meta.helm.sh/release-namespace: namespace-c
    creationTimestamp: "2022-07-14T20:18:24Z"
    generation: 1
    labels:
      app: compaas-echoserver-test
      pod-template-hash: f89ffd6c9
      release: echo
    name: echo-compaas-echoserver-test-f89ffd6c9
    namespace: namespace-c
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: echo-compaas-echoserver-test
      uid: 18364ed5-9334-4c15-9d2e-2fce197cff1a
    resourceVersion: "2060"
    uid: 0d45baf2-1845-4b5b-a207-068e996265e6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: compaas-echoserver-test
        pod-template-hash: f89ffd6c9
        release: echo
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: compaas-echoserver-test
          pod-template-hash: f89ffd6c9
          release: echo
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - echo-compaas-echoserver-test
                topologyKey: kubernetes.io/hostname
              weight: 100
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - echo-compaas-echoserver-test
                topologyKey: failure-domain.beta.kubernetes.io/zone
              weight: 50
        containers:
        - image: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
          imagePullPolicy: IfNotPresent
          name: echoserver
          resources:
            limits:
              cpu: 15m
              memory: 12Mi
            requests:
              cpu: 2m
              memory: 6Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certs
          - mountPath: /var/tmp/nginx
            name: nginx-tmp
          - mountPath: /run/nginx
            name: nginx-run
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: certs
        - emptyDir: {}
          name: nginx-tmp
        - emptyDir: {}
          name: nginx-run
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-07-14T20:11:14Z"
    generation: 1
    labels:
      io.cilium/app: operator
      name: cilium-operator
    name: cilium-operator
    namespace: kube-system
    resourceVersion: "989"
    uid: 7880b5f2-e588-4ff4-95ce-65d34fb7afb0
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        io.cilium/app: operator
        name: cilium-operator
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          io.cilium/app: operator
          name: cilium-operator
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: io.cilium/app
                  operator: In
                  values:
                  - operator
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --config-dir=/tmp/cilium/config-map
          - --debug=$(CILIUM_DEBUG)
          command:
          - cilium-operator-generic
          env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CILIUM_K8S_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CILIUM_DEBUG
            valueFrom:
              configMapKeyRef:
                key: debug
                name: cilium-config
                optional: true
          image: quay.io/cilium/operator-generic:v1.11.6@sha256:9f6063c7bcaede801a39315ec7c166309f6a6783e98665f6693939cf1701bc17
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9234
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: cilium-operator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/cilium/config-map
            name: cilium-config-path
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cilium-operator
        serviceAccountName: cilium-operator
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: cilium-config
          name: cilium-config-path
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2022-07-14T20:11:14Z"
      lastUpdateTime: "2022-07-14T20:11:14Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2022-07-14T20:11:14Z"
      lastUpdateTime: "2022-07-14T20:13:00Z"
      message: ReplicaSet "cilium-operator-5887f78bbb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-07-14T20:09:08Z"
    generation: 1
    labels:
      k8s-app: kube-dns
    name: coredns
    namespace: kube-system
    resourceVersion: "1104"
    uid: b6f1a4d0-56a9-4150-8e88-656221bc88b5
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: k8s.gcr.io/coredns/coredns:v1.8.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2022-07-14T20:13:18Z"
      lastUpdateTime: "2022-07-14T20:13:18Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2022-07-14T20:09:21Z"
      lastUpdateTime: "2022-07-14T20:13:19Z"
      message: ReplicaSet "coredns-6d4b75cb6d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-07-14T20:13:54Z"
    generation: 1
    labels:
      k8s-app: hubble-relay
    name: hubble-relay
    namespace: kube-system
    resourceVersion: "1375"
    uid: 4da5a46e-2552-40a1-bfa9-102e3ede194c
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: hubble-relay
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: hubble-relay
      spec:
        affinity:
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: k8s-app
                  operator: In
                  values:
                  - cilium
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: false
        containers:
        - args:
          - serve
          command:
          - hubble-relay
          image: quay.io/cilium/hubble-relay:v1.11.6@sha256:fd9034a2d04d5b973f1e8ed44f230ea195b89c37955ff32e34e5aa68f3ed675a
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: grpc
            timeoutSeconds: 1
          name: hubble-relay
          ports:
          - containerPort: 4245
            name: grpc
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: grpc
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/hubble-relay
            name: config
            readOnly: true
          - mountPath: /var/lib/hubble-relay/tls
            name: tls
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hubble-relay
        serviceAccountName: hubble-relay
        terminationGracePeriodSeconds: 0
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: config.yaml
              path: config.yaml
            name: hubble-relay-config
          name: config
        - name: tls
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: ca.crt
                  path: hubble-server-ca.crt
                - key: tls.crt
                  path: client.crt
                - key: tls.key
                  path: client.key
                name: hubble-relay-client-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2022-07-14T20:13:54Z"
      lastUpdateTime: "2022-07-14T20:13:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2022-07-14T20:13:54Z"
      lastUpdateTime: "2022-07-14T20:14:08Z"
      message: ReplicaSet "hubble-relay-959988db5" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"local-path-provisioner","namespace":"local-path-storage"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"local-path-provisioner"}},"template":{"metadata":{"labels":{"app":"local-path-provisioner"}},"spec":{"containers":[{"command":["local-path-provisioner","--debug","start","--helper-image","docker.io/kindest/local-path-helper:v20220512-507ff70b","--config","/etc/config/config.json"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"docker.io/kindest/local-path-provisioner:v0.0.22-kind.0","imagePullPolicy":"IfNotPresent","name":"local-path-provisioner","volumeMounts":[{"mountPath":"/etc/config/","name":"config-volume"}]}],"nodeSelector":{"kubernetes.io/os":"linux"},"serviceAccountName":"local-path-provisioner-service-account","tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/control-plane","operator":"Equal"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/master","operator":"Equal"}],"volumes":[{"configMap":{"name":"local-path-config"},"name":"config-volume"}]}}}}
    creationTimestamp: "2022-07-14T20:09:10Z"
    generation: 1
    name: local-path-provisioner
    namespace: local-path-storage
    resourceVersion: "1092"
    uid: d489f0c5-bcb8-4a27-8726-11ca3bb74617
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: local-path-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
      spec:
        containers:
        - command:
          - local-path-provisioner
          - --debug
          - start
          - --helper-image
          - docker.io/kindest/local-path-helper:v20220512-507ff70b
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Equal
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Equal
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2022-07-14T20:13:18Z"
      lastUpdateTime: "2022-07-14T20:13:18Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2022-07-14T20:09:21Z"
      lastUpdateTime: "2022-07-14T20:13:18Z"
      message: ReplicaSet "local-path-provisioner-9cd9bd544" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: echo
      meta.helm.sh/release-namespace: namespace-a
    creationTimestamp: "2022-07-14T20:15:33Z"
    generation: 1
    labels:
      app: compaas-echoserver-test
      app.kubernetes.io/managed-by: Helm
      chart: compaas-echoserver-test-2.1.1
      heritage: Helm
      release: echo
    name: echo-compaas-echoserver-test
    namespace: namespace-a
    resourceVersion: "1670"
    uid: 5521da30-c2fa-453c-9e79-74e4928aeed4
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: compaas-echoserver-test
        release: echo
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: compaas-echoserver-test
          release: echo
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - echo-compaas-echoserver-test
                topologyKey: kubernetes.io/hostname
              weight: 100
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - echo-compaas-echoserver-test
                topologyKey: failure-domain.beta.kubernetes.io/zone
              weight: 50
        containers:
        - image: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
          imagePullPolicy: IfNotPresent
          name: echoserver
          resources:
            limits:
              cpu: 15m
              memory: 12Mi
            requests:
              cpu: 2m
              memory: 6Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certs
          - mountPath: /var/tmp/nginx
            name: nginx-tmp
          - mountPath: /run/nginx
            name: nginx-run
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: certs
        - emptyDir: {}
          name: nginx-tmp
        - emptyDir: {}
          name: nginx-run
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2022-07-14T20:15:52Z"
      lastUpdateTime: "2022-07-14T20:15:52Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2022-07-14T20:15:33Z"
      lastUpdateTime: "2022-07-14T20:15:52Z"
      message: ReplicaSet "echo-compaas-echoserver-test-f89ffd6c9" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: echo
      meta.helm.sh/release-namespace: namespace-b
    creationTimestamp: "2022-07-14T20:15:42Z"
    generation: 1
    labels:
      app: compaas-echoserver-test
      app.kubernetes.io/managed-by: Helm
      chart: compaas-echoserver-test-2.1.1
      heritage: Helm
      release: echo
    name: echo-compaas-echoserver-test
    namespace: namespace-b
    resourceVersion: "1674"
    uid: f91d84c4-d398-43b6-9468-2859287d1df3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: compaas-echoserver-test
        release: echo
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: compaas-echoserver-test
          release: echo
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - echo-compaas-echoserver-test
                topologyKey: kubernetes.io/hostname
              weight: 100
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - echo-compaas-echoserver-test
                topologyKey: failure-domain.beta.kubernetes.io/zone
              weight: 50
        containers:
        - image: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
          imagePullPolicy: IfNotPresent
          name: echoserver
          resources:
            limits:
              cpu: 15m
              memory: 12Mi
            requests:
              cpu: 2m
              memory: 6Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certs
          - mountPath: /var/tmp/nginx
            name: nginx-tmp
          - mountPath: /run/nginx
            name: nginx-run
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: certs
        - emptyDir: {}
          name: nginx-tmp
        - emptyDir: {}
          name: nginx-run
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2022-07-14T20:15:52Z"
      lastUpdateTime: "2022-07-14T20:15:52Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2022-07-14T20:15:42Z"
      lastUpdateTime: "2022-07-14T20:15:52Z"
      message: ReplicaSet "echo-compaas-echoserver-test-f89ffd6c9" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: echo
      meta.helm.sh/release-namespace: namespace-c
    creationTimestamp: "2022-07-14T20:18:24Z"
    generation: 1
    labels:
      app: compaas-echoserver-test
      app.kubernetes.io/managed-by: Helm
      chart: compaas-echoserver-test-2.1.1
      heritage: Helm
      release: echo
    name: echo-compaas-echoserver-test
    namespace: namespace-c
    resourceVersion: "2063"
    uid: 18364ed5-9334-4c15-9d2e-2fce197cff1a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: compaas-echoserver-test
        release: echo
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: compaas-echoserver-test
          release: echo
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - echo-compaas-echoserver-test
                topologyKey: kubernetes.io/hostname
              weight: 100
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - echo-compaas-echoserver-test
                topologyKey: failure-domain.beta.kubernetes.io/zone
              weight: 50
        containers:
        - image: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
          imagePullPolicy: IfNotPresent
          name: echoserver
          resources:
            limits:
              cpu: 15m
              memory: 12Mi
            requests:
              cpu: 2m
              memory: 6Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certs
          - mountPath: /var/tmp/nginx
            name: nginx-tmp
          - mountPath: /run/nginx
            name: nginx-run
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: certs
        - emptyDir: {}
          name: nginx-tmp
        - emptyDir: {}
          name: nginx-run
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2022-07-14T20:18:41Z"
      lastUpdateTime: "2022-07-14T20:18:41Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2022-07-14T20:18:24Z"
      lastUpdateTime: "2022-07-14T20:18:41Z"
      message: ReplicaSet "echo-compaas-echoserver-test-f89ffd6c9" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2022-07-14T20:11:14Z"
    generation: 1
    labels:
      k8s-app: cilium
    name: cilium
    namespace: kube-system
    resourceVersion: "1294"
    uid: 80b4a8a3-e331-40bf-a7d8-40d406ca20d2
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: cilium
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: cilium
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: In
                  values:
                  - linux
              - matchExpressions:
                - key: beta.kubernetes.io/os
                  operator: In
                  values:
                  - linux
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: k8s-app
                  operator: In
                  values:
                  - cilium
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --config-dir=/tmp/cilium/config-map
          command:
          - cilium-agent
          env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CILIUM_K8S_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CILIUM_CLUSTERMESH_CONFIG
            value: /var/lib/cilium/clustermesh/
          - name: CILIUM_CNI_CHAINING_MODE
            valueFrom:
              configMapKeyRef:
                key: cni-chaining-mode
                name: cilium-config
                optional: true
          - name: CILIUM_CUSTOM_CNI_CONF
            valueFrom:
              configMapKeyRef:
                key: custom-cni-conf
                name: cilium-config
                optional: true
          image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
          imagePullPolicy: IfNotPresent
          lifecycle:
            postStart:
              exec:
                command:
                - /cni-install.sh
                - --enable-debug=false
                - --cni-exclusive=true
            preStop:
              exec:
                command:
                - /cni-uninstall.sh
          livenessProbe:
            failureThreshold: 10
            httpGet:
              host: 127.0.0.1
              httpHeaders:
              - name: brief
                value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: cilium-agent
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              httpHeaders:
              - name: brief
                value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            privileged: true
          startupProbe:
            failureThreshold: 105
            httpGet:
              host: 127.0.0.1
              httpHeaders:
              - name: brief
                value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /sys/fs/bpf
            mountPropagation: Bidirectional
            name: bpf-maps
          - mountPath: /var/run/cilium
            name: cilium-run
          - mountPath: /host/opt/cni/bin
            name: cni-path
          - mountPath: /host/etc/cni/net.d
            name: etc-cni-netd
          - mountPath: /var/lib/cilium/clustermesh
            name: clustermesh-secrets
            readOnly: true
          - mountPath: /tmp/cilium/config-map
            name: cilium-config-path
            readOnly: true
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /var/lib/cilium/tls/hubble
            name: hubble-tls
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - command:
          - sh
          - -ec
          - |
            cp /usr/bin/cilium-mount /hostbin/cilium-mount;
            nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
            rm /hostbin/cilium-mount
          env:
          - name: CGROUP_ROOT
            value: /run/cilium/cgroupv2
          - name: BIN_PATH
            value: /opt/cni/bin
          image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
          imagePullPolicy: IfNotPresent
          name: mount-cgroup
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /hostproc
            name: hostproc
          - mountPath: /hostbin
            name: cni-path
        - command:
          - /init-container.sh
          env:
          - name: CILIUM_ALL_STATE
            valueFrom:
              configMapKeyRef:
                key: clean-cilium-state
                name: cilium-config
                optional: true
          - name: CILIUM_BPF_STATE
            valueFrom:
              configMapKeyRef:
                key: clean-cilium-bpf-state
                name: cilium-config
                optional: true
          image: quay.io/cilium/cilium:v1.11.6@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
          imagePullPolicy: IfNotPresent
          name: clean-cilium-state
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /sys/fs/bpf
            name: bpf-maps
          - mountPath: /run/cilium/cgroupv2
            mountPropagation: HostToContainer
            name: cilium-cgroup
          - mountPath: /var/run/cilium
            name: cilium-run
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cilium
        serviceAccountName: cilium
        terminationGracePeriodSeconds: 1
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /var/run/cilium
            type: DirectoryOrCreate
          name: cilium-run
        - hostPath:
            path: /sys/fs/bpf
            type: DirectoryOrCreate
          name: bpf-maps
        - hostPath:
            path: /proc
            type: Directory
          name: hostproc
        - hostPath:
            path: /run/cilium/cgroupv2
            type: DirectoryOrCreate
          name: cilium-cgroup
        - hostPath:
            path: /opt/cni/bin
            type: DirectoryOrCreate
          name: cni-path
        - hostPath:
            path: /etc/cni/net.d
            type: DirectoryOrCreate
          name: etc-cni-netd
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - name: clustermesh-secrets
          secret:
            defaultMode: 256
            optional: true
            secretName: cilium-clustermesh
        - configMap:
            defaultMode: 420
            name: cilium-config
          name: cilium-config-path
        - name: hubble-tls
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: ca.crt
                  path: client-ca.crt
                - key: tls.crt
                  path: server.crt
                - key: tls.key
                  path: server.key
                name: hubble-server-certs
                optional: true
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 2
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2022-07-14T20:09:08Z"
    generation: 1
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "494"
    uid: 61c216a8-b0ee-4e1d-853b-fae68d2b0626
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-proxy
      spec:
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --config=/var/lib/kube-proxy/config.conf
          - --hostname-override=$(NODE_NAME)
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: HTTPS_PROXY
            value: http://135.245.192.7:8000
          - name: https_proxy
            value: http://135.245.192.7:8000
          - name: NO_PROXY
            value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
          - name: no_proxy
            value: 172.18.0.0/16,fc00:f853:ccd:e793::/64,10.96.0.0/16,10.244.0.0/16,kind-control-plane,kind-worker,kind-worker2,kind-worker3,.svc,.svc.cluster,.svc.cluster.local
          - name: HTTP_PROXY
            value: http://135.245.192.7:8000
          - name: http_proxy
            value: http://135.245.192.7:8000
          image: k8s.gcr.io/kube-proxy:v1.24.0
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2022-07-14T20:09:06Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "195"
    uid: 9bf1d251-4029-4e1c-8b3a-9d75ab86e35c
  spec:
    clusterIP: 10.96.0.1
    clusterIPs:
    - 10.96.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2022-07-14T20:13:53Z"
    labels:
      k8s-app: cilium
    name: hubble-peer
    namespace: kube-system
    resourceVersion: "1308"
    uid: 1953ab15-e510-459a-afbb-4a5e4376df81
  spec:
    clusterIP: 10.96.112.33
    clusterIPs:
    - 10.96.112.33
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: peer-service
      port: 443
      protocol: TCP
      targetPort: 4244
    selector:
      k8s-app: cilium
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2022-07-14T20:13:54Z"
    labels:
      k8s-app: hubble-relay
    name: hubble-relay
    namespace: kube-system
    resourceVersion: "1318"
    uid: 767171a8-c147-499e-a97b-117199f63fbf
  spec:
    clusterIP: 10.96.170.85
    clusterIPs:
    - 10.96.170.85
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: 4245
    selector:
      k8s-app: hubble-relay
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2022-07-14T20:09:08Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "246"
    uid: e997e12b-3242-442d-a292-528a5335f058
  spec:
    clusterIP: 10.96.0.10
    clusterIPs:
    - 10.96.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: echo
      meta.helm.sh/release-namespace: namespace-a
    creationTimestamp: "2022-07-14T20:15:33Z"
    labels:
      app: compaas-echoserver-test
      app.kubernetes.io/managed-by: Helm
      chart: compaas-echoserver-test-2.1.1
      heritage: Helm
      release: echo
    name: echo-compaas-echoserver-test
    namespace: namespace-a
    resourceVersion: "1555"
    uid: c575db64-3750-4e5d-8204-455a1d5f509b
  spec:
    clusterIP: 10.96.123.34
    clusterIPs:
    - 10.96.123.34
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: port80
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: port443
      port: 443
      protocol: TCP
      targetPort: 8443
    selector:
      app: compaas-echoserver-test
      release: echo
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: echo
      meta.helm.sh/release-namespace: namespace-b
    creationTimestamp: "2022-07-14T20:15:42Z"
    labels:
      app: compaas-echoserver-test
      app.kubernetes.io/managed-by: Helm
      chart: compaas-echoserver-test-2.1.1
      heritage: Helm
      release: echo
    name: echo-compaas-echoserver-test
    namespace: namespace-b
    resourceVersion: "1596"
    uid: dfb39dfc-4c67-4084-a03f-ed961c7f526e
  spec:
    clusterIP: 10.96.112.132
    clusterIPs:
    - 10.96.112.132
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: port80
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: port443
      port: 443
      protocol: TCP
      targetPort: 8443
    selector:
      app: compaas-echoserver-test
      release: echo
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: echo
      meta.helm.sh/release-namespace: namespace-c
    creationTimestamp: "2022-07-14T20:18:24Z"
    labels:
      app: compaas-echoserver-test
      app.kubernetes.io/managed-by: Helm
      chart: compaas-echoserver-test-2.1.1
      heritage: Helm
      release: echo
    name: echo-compaas-echoserver-test
    namespace: namespace-c
    resourceVersion: "1997"
    uid: 57b3e928-e6e4-424c-809a-b6e120d23319
  spec:
    clusterIP: 10.96.107.173
    clusterIPs:
    - 10.96.107.173
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: port80
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: port443
      port: 443
      protocol: TCP
      targetPort: 8443
    selector:
      app: compaas-echoserver-test
      release: echo
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      io.cilium.network.ipv4-cilium-host: 10.244.0.8
      io.cilium.network.ipv4-health-ip: 10.244.0.10
      io.cilium.network.ipv4-pod-cidr: 10.244.0.0/24
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2022-07-14T20:09:07Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: kind-control-plane
      kubernetes.io/os: linux
      node-role.kubernetes.io/control-plane: ""
      node.kubernetes.io/exclude-from-external-load-balancers: ""
    name: kind-control-plane
    resourceVersion: "5060"
    uid: c2d1e453-3eb5-45e3-955a-8d9efe36246b
  spec:
    podCIDR: 10.244.0.0/24
    podCIDRs:
    - 10.244.0.0/24
    providerID: kind://docker/kind/kind-control-plane
    taints:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
  status:
    addresses:
    - address: 172.18.0.5
      type: InternalIP
    - address: kind-control-plane
      type: Hostname
    allocatable:
      cpu: "4"
      ephemeral-storage: 40972512Ki
      hugepages-2Mi: "0"
      memory: 8142264Ki
      pods: "110"
    capacity:
      cpu: "4"
      ephemeral-storage: 40972512Ki
      hugepages-2Mi: "0"
      memory: 8142264Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2022-07-14T20:13:04Z"
      lastTransitionTime: "2022-07-14T20:13:04Z"
      message: Cilium is running on this node
      reason: CiliumIsUp
      status: "False"
      type: NetworkUnavailable
    - lastHeartbeatTime: "2022-07-14T20:43:49Z"
      lastTransitionTime: "2022-07-14T20:09:06Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2022-07-14T20:43:49Z"
      lastTransitionTime: "2022-07-14T20:09:06Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2022-07-14T20:43:49Z"
      lastTransitionTime: "2022-07-14T20:09:06Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2022-07-14T20:43:49Z"
      lastTransitionTime: "2022-07-14T20:12:53Z"
      message: kubelet is posting ready status
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      sizeBytes: 159114031
    - names:
      - docker.io/library/import-2022-05-19@sha256:654122cbd980cb4a8417347b8c84ecdbc81a735d3d01409bec6f37ef293511ca
      - k8s.gcr.io/kube-proxy:v1.24.0
      sizeBytes: 111847276
    - names:
      - k8s.gcr.io/etcd:3.5.3-0
      sizeBytes: 102143581
    - names:
      - docker.io/library/import-2022-05-19@sha256:5611470386fb81d6d169bb37667ae1f9fa884631b265b54d8d9601d9338aad18
      - k8s.gcr.io/kube-apiserver:v1.24.0
      sizeBytes: 77273570
    - names:
      - docker.io/library/import-2022-05-19@sha256:948ca7c1da4b6108193f4dae34f2e94cfdaa901ab8876ba4009cc024673d2af4
      - k8s.gcr.io/kube-controller-manager:v1.24.0
      sizeBytes: 65554548
    - names:
      - docker.io/library/import-2022-05-19@sha256:289df0671b753c90e6b717b92ada9af9bcb48678d59affc8cc27eef4f01e9251
      - k8s.gcr.io/kube-scheduler:v1.24.0
      sizeBytes: 52332660
    - names:
      - docker.io/kindest/kindnetd:v20220510-4929dd75
      sizeBytes: 45239873
    - names:
      - docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
      sizeBytes: 17375346
    - names:
      - k8s.gcr.io/coredns/coredns:v1.8.6
      sizeBytes: 13585107
    - names:
      - docker.io/kindest/local-path-helper:v20220512-507ff70b
      sizeBytes: 2859518
    - names:
      - k8s.gcr.io/pause:3.6
      sizeBytes: 301773
    nodeInfo:
      architecture: amd64
      bootID: 9e0bd7cf-8989-457e-9010-183f6a48c295
      containerRuntimeVersion: containerd://1.6.4
      kernelVersion: 5.13.0-52-generic
      kubeProxyVersion: v1.24.0
      kubeletVersion: v1.24.0
      machineID: d8c06376d7504caca1c817f570b16fed
      operatingSystem: linux
      osImage: Ubuntu 21.10
      systemUUID: 0798a679-3dd3-4ca6-b80e-e8072b5504f6
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      io.cilium.network.ipv4-cilium-host: 10.244.1.99
      io.cilium.network.ipv4-health-ip: 10.244.1.219
      io.cilium.network.ipv4-pod-cidr: 10.244.1.0/24
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2022-07-14T20:09:25Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: kind-worker
      kubernetes.io/os: linux
    name: kind-worker
    resourceVersion: "5408"
    uid: 4caeae9b-56be-407b-a36c-e5a470bb5d6b
  spec:
    podCIDR: 10.244.1.0/24
    podCIDRs:
    - 10.244.1.0/24
    providerID: kind://docker/kind/kind-worker
  status:
    addresses:
    - address: 172.18.0.3
      type: InternalIP
    - address: kind-worker
      type: Hostname
    allocatable:
      cpu: "4"
      ephemeral-storage: 40972512Ki
      hugepages-2Mi: "0"
      memory: 8142264Ki
      pods: "110"
    capacity:
      cpu: "4"
      ephemeral-storage: 40972512Ki
      hugepages-2Mi: "0"
      memory: 8142264Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2022-07-14T20:13:06Z"
      lastTransitionTime: "2022-07-14T20:13:06Z"
      message: Cilium is running on this node
      reason: CiliumIsUp
      status: "False"
      type: NetworkUnavailable
    - lastHeartbeatTime: "2022-07-14T20:46:44Z"
      lastTransitionTime: "2022-07-14T20:09:25Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2022-07-14T20:46:44Z"
      lastTransitionTime: "2022-07-14T20:09:25Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2022-07-14T20:46:44Z"
      lastTransitionTime: "2022-07-14T20:09:25Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2022-07-14T20:46:44Z"
      lastTransitionTime: "2022-07-14T20:13:02Z"
      message: kubelet is posting ready status
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      sizeBytes: 159114031
    - names:
      - docker.io/library/import-2022-05-19@sha256:654122cbd980cb4a8417347b8c84ecdbc81a735d3d01409bec6f37ef293511ca
      - k8s.gcr.io/kube-proxy:v1.24.0
      sizeBytes: 111847276
    - names:
      - k8s.gcr.io/etcd:3.5.3-0
      sizeBytes: 102143581
    - names:
      - docker.io/library/import-2022-05-19@sha256:5611470386fb81d6d169bb37667ae1f9fa884631b265b54d8d9601d9338aad18
      - k8s.gcr.io/kube-apiserver:v1.24.0
      sizeBytes: 77273570
    - names:
      - docker.io/library/import-2022-05-19@sha256:948ca7c1da4b6108193f4dae34f2e94cfdaa901ab8876ba4009cc024673d2af4
      - k8s.gcr.io/kube-controller-manager:v1.24.0
      sizeBytes: 65554548
    - names:
      - docker.io/library/import-2022-05-19@sha256:289df0671b753c90e6b717b92ada9af9bcb48678d59affc8cc27eef4f01e9251
      - k8s.gcr.io/kube-scheduler:v1.24.0
      sizeBytes: 52332660
    - names:
      - docker.io/kindest/kindnetd:v20220510-4929dd75
      sizeBytes: 45239873
    - names:
      - docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
      sizeBytes: 17375346
    - names:
      - k8s.gcr.io/coredns/coredns:v1.8.6
      sizeBytes: 13585107
    - names:
      - csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test@sha256:1361ea03466a6ea07a1607d73cae74cb3214961411e36867a5ffb113b7e15cd0
      - csf-docker-delivered.repo.lab.pl.alcatel-lucent.com/csf/paas/development/compaas-echoserver-test:2.1.1-229-release
      sizeBytes: 9309045
    - names:
      - docker.io/kindest/local-path-helper:v20220512-507ff70b
      sizeBytes: 2859518
    - names:
      - k8s.gcr.io/pause:3.6
      sizeBytes: 301773
    nodeInfo:
      architecture: amd64
      bootID: 9e0bd7cf-8989-457e-9010-183f6a48c295
      containerRuntimeVersion: containerd://1.6.4
      kernelVersion: 5.13.0-52-generic
      kubeProxyVersion: v1.24.0
      kubeletVersion: v1.24.0
      machineID: eb153a714bd3475da14b387f11573b6a
      operatingSystem: linux
      osImage: Ubuntu 21.10
      systemUUID: e58c91a6-58ae-41f1-8854-9a011acc6d40
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      io.cilium.network.ipv4-cilium-host: 10.244.2.184
      io.cilium.network.ipv4-health-ip: 10.244.2.51
      io.cilium.network.ipv4-pod-cidr: 10.244.2.0/24
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2022-07-14T20:09:26Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: kind-worker2
      kubernetes.io/os: linux
    name: kind-worker2
    resourceVersion: "5109"
    uid: 2ce78e74-bde5-40ca-a2e6-ce8c6494fbca
  spec:
    podCIDR: 10.244.2.0/24
    podCIDRs:
    - 10.244.2.0/24
    providerID: kind://docker/kind/kind-worker2
  status:
    addresses:
    - address: 172.18.0.2
      type: InternalIP
    - address: kind-worker2
      type: Hostname
    allocatable:
      cpu: "4"
      ephemeral-storage: 40972512Ki
      hugepages-2Mi: "0"
      memory: 8142264Ki
      pods: "110"
    capacity:
      cpu: "4"
      ephemeral-storage: 40972512Ki
      hugepages-2Mi: "0"
      memory: 8142264Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2022-07-14T20:13:04Z"
      lastTransitionTime: "2022-07-14T20:13:04Z"
      message: Cilium is running on this node
      reason: CiliumIsUp
      status: "False"
      type: NetworkUnavailable
    - lastHeartbeatTime: "2022-07-14T20:44:12Z"
      lastTransitionTime: "2022-07-14T20:09:26Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2022-07-14T20:44:12Z"
      lastTransitionTime: "2022-07-14T20:09:26Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2022-07-14T20:44:12Z"
      lastTransitionTime: "2022-07-14T20:09:26Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2022-07-14T20:44:12Z"
      lastTransitionTime: "2022-07-14T20:13:01Z"
      message: kubelet is posting ready status
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      sizeBytes: 159114031
    - names:
      - docker.io/library/import-2022-05-19@sha256:654122cbd980cb4a8417347b8c84ecdbc81a735d3d01409bec6f37ef293511ca
      - k8s.gcr.io/kube-proxy:v1.24.0
      sizeBytes: 111847276
    - names:
      - k8s.gcr.io/etcd:3.5.3-0
      sizeBytes: 102143581
    - names:
      - docker.io/library/import-2022-05-19@sha256:5611470386fb81d6d169bb37667ae1f9fa884631b265b54d8d9601d9338aad18
      - k8s.gcr.io/kube-apiserver:v1.24.0
      sizeBytes: 77273570
    - names:
      - docker.io/library/import-2022-05-19@sha256:948ca7c1da4b6108193f4dae34f2e94cfdaa901ab8876ba4009cc024673d2af4
      - k8s.gcr.io/kube-controller-manager:v1.24.0
      sizeBytes: 65554548
    - names:
      - docker.io/library/import-2022-05-19@sha256:289df0671b753c90e6b717b92ada9af9bcb48678d59affc8cc27eef4f01e9251
      - k8s.gcr.io/kube-scheduler:v1.24.0
      sizeBytes: 52332660
    - names:
      - docker.io/kindest/kindnetd:v20220510-4929dd75
      sizeBytes: 45239873
    - names:
      - docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
      sizeBytes: 17375346
    - names:
      - quay.io/cilium/operator-generic@sha256:9f6063c7bcaede801a39315ec7c166309f6a6783e98665f6693939cf1701bc17
      sizeBytes: 16741278
    - names:
      - k8s.gcr.io/coredns/coredns:v1.8.6
      sizeBytes: 13585107
    - names:
      - docker.io/kindest/local-path-helper:v20220512-507ff70b
      sizeBytes: 2859518
    - names:
      - k8s.gcr.io/pause:3.6
      sizeBytes: 301773
    nodeInfo:
      architecture: amd64
      bootID: 9e0bd7cf-8989-457e-9010-183f6a48c295
      containerRuntimeVersion: containerd://1.6.4
      kernelVersion: 5.13.0-52-generic
      kubeProxyVersion: v1.24.0
      kubeletVersion: v1.24.0
      machineID: 8e8744e900b84d1eaa1b9724ca49660d
      operatingSystem: linux
      osImage: Ubuntu 21.10
      systemUUID: 3c93667a-0345-4a82-af45-6766e7b6d2ee
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      io.cilium.network.ipv4-cilium-host: 10.244.3.154
      io.cilium.network.ipv4-health-ip: 10.244.3.155
      io.cilium.network.ipv4-pod-cidr: 10.244.3.0/24
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2022-07-14T20:09:28Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: kind-worker3
      kubernetes.io/os: linux
    name: kind-worker3
    resourceVersion: "5230"
    uid: 56e61e0d-8c4a-4311-a5d4-a32a32a1e2e0
  spec:
    podCIDR: 10.244.3.0/24
    podCIDRs:
    - 10.244.3.0/24
    providerID: kind://docker/kind/kind-worker3
  status:
    addresses:
    - address: 172.18.0.4
      type: InternalIP
    - address: kind-worker3
      type: Hostname
    allocatable:
      cpu: "4"
      ephemeral-storage: 40972512Ki
      hugepages-2Mi: "0"
      memory: 8142264Ki
      pods: "110"
    capacity:
      cpu: "4"
      ephemeral-storage: 40972512Ki
      hugepages-2Mi: "0"
      memory: 8142264Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2022-07-14T20:13:04Z"
      lastTransitionTime: "2022-07-14T20:13:04Z"
      message: Cilium is running on this node
      reason: CiliumIsUp
      status: "False"
      type: NetworkUnavailable
    - lastHeartbeatTime: "2022-07-14T20:45:12Z"
      lastTransitionTime: "2022-07-14T20:09:28Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2022-07-14T20:45:12Z"
      lastTransitionTime: "2022-07-14T20:09:28Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2022-07-14T20:45:12Z"
      lastTransitionTime: "2022-07-14T20:09:28Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2022-07-14T20:45:12Z"
      lastTransitionTime: "2022-07-14T20:13:53Z"
      message: kubelet is posting ready status
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - quay.io/cilium/cilium@sha256:f7f93c26739b6641a3fa3d76b1e1605b15989f25d06625260099e01c8243f54c
      sizeBytes: 159114031
    - names:
      - docker.io/library/import-2022-05-19@sha256:654122cbd980cb4a8417347b8c84ecdbc81a735d3d01409bec6f37ef293511ca
      - k8s.gcr.io/kube-proxy:v1.24.0
      sizeBytes: 111847276
    - names:
      - k8s.gcr.io/etcd:3.5.3-0
      sizeBytes: 102143581
    - names:
      - docker.io/library/import-2022-05-19@sha256:5611470386fb81d6d169bb37667ae1f9fa884631b265b54d8d9601d9338aad18
      - k8s.gcr.io/kube-apiserver:v1.24.0
      sizeBytes: 77273570
    - names:
      - docker.io/library/import-2022-05-19@sha256:948ca7c1da4b6108193f4dae34f2e94cfdaa901ab8876ba4009cc024673d2af4
      - k8s.gcr.io/kube-controller-manager:v1.24.0
      sizeBytes: 65554548
    - names:
      - docker.io/library/import-2022-05-19@sha256:289df0671b753c90e6b717b92ada9af9bcb48678d59affc8cc27eef4f01e9251
      - k8s.gcr.io/kube-scheduler:v1.24.0
      sizeBytes: 52332660
    - names:
      - docker.io/kindest/kindnetd:v20220510-4929dd75
      sizeBytes: 45239873
    - names:
      - docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
      sizeBytes: 17375346
    - names:
      - quay.io/cilium/hubble-relay@sha256:fd9034a2d04d5b973f1e8ed44f230ea195b89c37955ff32e34e5aa68f3ed675a
      sizeBytes: 13823004
    - names:
      - k8s.gcr.io/coredns/coredns:v1.8.6
      sizeBytes: 13585107
    - names:
      - docker.io/kindest/local-path-helper:v20220512-507ff70b
      sizeBytes: 2859518
    - names:
      - k8s.gcr.io/pause:3.6
      sizeBytes: 301773
    nodeInfo:
      architecture: amd64
      bootID: 9e0bd7cf-8989-457e-9010-183f6a48c295
      containerRuntimeVersion: containerd://1.6.4
      kernelVersion: 5.13.0-52-generic
      kubeProxyVersion: v1.24.0
      kubeletVersion: v1.24.0
      machineID: 93eafd863a9b4fbfb03d165375e6ff24
      operatingSystem: linux
      osImage: Ubuntu 21.10
      systemUUID: bb3ae09f-6ea3-49ca-8792-c1eb11acdec7
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
